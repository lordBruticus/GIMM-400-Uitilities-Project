{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Utilities.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordBruticus/GIMM-400-Uitilities-Project/blob/master/Utilities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntd2ThKszrfw",
        "colab_type": "text"
      },
      "source": [
        "**Hello World**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27WDg-bEz0yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Console.WriteLine(\"Hello world\");\n",
        "\n",
        "string aFriend = \"Bill\";\n",
        "Console.WriteLine(aFriend);\n",
        "\n",
        "//aFriend = \"Maria\";\n",
        "//Console.WriteLine(aFriend);\n",
        "//Console.WriteLine(\"Hello \" + aFriend);\n",
        "//Console.WriteLine($\"Hello  {aFriend}\");\n",
        "\n",
        "//string firstFriend = \"Maria\";\n",
        "//string secondFriend = \"Sage\";\n",
        "//Console.WriteLine($\"My friends are {firstFriend} and {secondFriend}\");\n",
        "\n",
        "//Console.WriteLine($\"My friends are {secondFriend} and {firstFriend.Length} letters.\");\n",
        "//Console.WriteLine($\"My friends are {firstFriend} and {secondFriend.Length} letters.\");\n",
        "\n",
        "string greeting = \"    Hello World    \";\n",
        "Console.WritieLine($\"[{greeting}]\");\n",
        "\n",
        "string trimmedGreeting = greeting.TrimStart();\n",
        "Console.WriteLine($\"[{trimmedGreeting}]\");\n",
        "\n",
        "trimmedGreeting = greeting.TrimEnd();\n",
        "Console.WriteLine($\"[{trimmedGreeting}]\");\n",
        "\n",
        "trimmedGreeting = greeting.Trim();\n",
        "Console.WriteLine($\"[{trimmedGreeting}]\");\n",
        "\n",
        "string sayHello = \"Hello World!\";\n",
        "Console.WriteLine(sayHello);\n",
        "sayHello = sayHello.Replace(\"Hello\", \"Greetings\");\n",
        "Console.WriteLine(sayHello);\n",
        "\n",
        "Console.WriteLine(sayHello.ToUpper());\n",
        "Console.WriteLine(sayHello.ToLower());\n",
        "\n",
        "string songLyrics = \"You say goodbye, and I say hello\";\n",
        "Console.WriteLine(songLyris.Contains(\"goodbye\"));\n",
        "Console.WriteLine(songLyris.Contains(\"greetings\"));\n",
        "\n",
        "string songLyrics = \"You say goodbye, and I say hello\";\n",
        "Console.WriteLine(songLyrics.StartsWith(\"You\"));\n",
        "Console.WriteLine(songLyrics.StartsWith(\"goodbye\"));\n",
        "\n",
        "Console.WriteLine(songLyrics.EndsWith(\"hello\"));\n",
        "Console.WriteLine(songLyrics.EndsWith(\"goodbye\"));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQGlJZ7NxUbH",
        "colab_type": "text"
      },
      "source": [
        "**Neural Networks: Simple to Complex**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_n5sU5rtdt",
        "colab_type": "code",
        "outputId": "213587a6-8dcf-499b-9a97-5af615c2569c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Simple Feedforward Artificial Neural Network: no external libraries like tensorflow, pandas, numpy, or keras:\n",
        "# No backpropogation or optimization:\n",
        "\n",
        "epoch = 5 #number of times you will run the neural net through the entire training set FIX THIS\n",
        "lr = .00001 #learning rate FIX THIS\n",
        "bias = .1 #value of bias FIX THIS\n",
        "weights = [random.random(),random.random(),random.random()] #weights generated in a list (3 weights in total for 2 neurons and the bias)\n",
        "print(\"Weights before training: \" + str(weights))\n",
        "\n",
        "def NeuralNet(weights, inputNeuron1, inputNeuron2, expectedOutput):\n",
        "  outputActual = inputNeuron1*weights[0]+inputNeuron2*weights[1]+bias*weights[2]\n",
        "  outputActual = ApplyActivation(outputActual)\n",
        "  error = expectedOutput - outputActual #simple error measurement\n",
        "  #inference: here we train the hidden layer which is just an array of weights.\n",
        "  weights[0] += error * inputNeuron1 * lr\n",
        "  weights[1] += error * inputNeuron2 * lr\n",
        "  weights[2] += error * bias * lr\n",
        "def ApplyActivation(actInput):\n",
        "  if actInput > 0 : #activation function (here Heaviside because we want a 0 or 1 answer)\n",
        "     actInput = 1\n",
        "  else :\n",
        "     actInput = 0\n",
        "  return actInput\n",
        "\n",
        "\n",
        "for i in range(epoch):\n",
        "  NeuralNet(weights, 1,1,1) #True or true input should return true\n",
        "  NeuralNet(weights, 1,0,1) #True or false input should return true\n",
        "  NeuralNet(weights, 0,1,1) #False or true input should return true\n",
        "  NeuralNet(weights, 0,0,0) #False or false input should return false\n",
        "  \n",
        "print(\"Weights after training: \" + str(weights))\n",
        "x = int(input())\n",
        "y = int(input())\n",
        "output = x*weights[0] + y*weights[1] + bias*weights[2]\n",
        "if output > 0 : #activation function\n",
        "   output = 1\n",
        "else :\n",
        "   output = 0\n",
        "print(x, \"or\", y, \"is : \", output)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights before training: [0.12417331511991114, 0.2791836790111395, 0.5857592714582879]\n",
            "Weights after training: [0.12417331511991114, 0.2791836790111395, 0.5857542714582877]\n",
            "0\n",
            "0\n",
            "0 or 0 is :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEZRWMWkeGe",
        "colab_type": "code",
        "outputId": "e1b5d687-5e89-48ca-a039-1a632da512dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "import numpy as np #import numpy the linear algebra library\n",
        "\n",
        "# sigmoid function - introduces non-linearity into the neural net (not 1-1 input output)\n",
        "#sigmoid maps to a value between 0 and 1 to convert numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "    if(deriv==True): # when true, this maps the derivative of the sigmoid function - so we can get the slope at a given point\n",
        "        return x*(1-x)\n",
        "    return 1/(1+np.exp(-x))\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "epochs = 10000\n",
        "weights = np.array([[0,0,0,0]]).T\n",
        "# initialize weights randomly with mean 0\n",
        "#we only have an input and output layer, so we only need on matrix of wieghts to connect them with dimensions (3,1) 3 inputs to 1 output\n",
        "#if we wanted to change the number of inputs we would also change this (3,1) to reflect inputs, but we generally want 1 output since we're doing a regresion neural net\n",
        "#syn0 = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "def neuralNet(l0, epochs, predict, userTest=[0,0,0], testWeights=[0,0,0,0]):\n",
        "  if(not predict):\n",
        "    for iter in range(epochs):\n",
        "\n",
        "      # forward propagation - the prediction step, we let the network try to predict it's output and measure the error\n",
        "      syn0 = testWeights\n",
        "      #this is the 'weighted sum step where we use the dot or cross product of two matrices to output a matrix that has the multiplied and summed total of the weights and values for that row and column combination, and then we convert that back into a number since we get a probability with the sigmoid function'\n",
        "      #the dimensions going on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "      l1 = nonlin(np.dot(l0,syn0)) #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "      \n",
        "\n",
        "      # compare our guess - l1 - to the actual answer - y -\n",
        "      l1_error = y - l1\n",
        "      if(iter % 999 == 0):\n",
        "        print('error before slope of sigmoid of values in l1')\n",
        "        print(l1_error)\n",
        "        print(\"l1 itself (dot product of l0 and weight (weighted sum))\")\n",
        "        print(l1)\n",
        "    \n",
        "\n",
        "      # multiply how much we missed by the \n",
        "      # slope of the sigmoid at the values in l1 - basically we multiply elementwise a 4,1 matrix with another 4,1 matrix of its sigmoid derivatives thereby reducing the error of high confidence predictions.\n",
        "      # if the network has a very confident guess (slope is very shallow or close to 0), we leave it alone with the multiplication here but if it's closer to .5 then we heavily update the guess with this multiplication\n",
        "      l1_delta = l1_error * nonlin(l1,True)\n",
        "      if(iter%999 == 0):\n",
        "        print('error after slope of sigmoid of values in l1 is multiplied times l1 error')\n",
        "        print(l1_delta)\n",
        "        \n",
        "    \n",
        "\n",
        "     # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "      syn0 += np.dot(l0.T,l1_delta)\n",
        "    return syn0,l1\n",
        "    \n",
        "  if predict:           \n",
        "          \n",
        "    syn0 = testWeights\n",
        "    total = 0    #the dimensions goig on in the next step are: (4X3)dot(3X1) = (4X1) so everything in the matrix gets multplied and hte result is a matix with the number of rows in first matrix and number of columns in second\n",
        "    for i in range(len(userTest)):\n",
        "      #multiplies 10 input by syn0 weights, then passes it through our sigmoid function to convert the numbers to probabilities \n",
        "    #print(\"hey\" + str(l1))\n",
        "      total += userTest[i] * testWeights[i]\n",
        "        # compare our guess - l1 - to the actual answer - y -\n",
        "        # l1_error = y - l1\n",
        "        #l1_delta = l1_error * nonlin(l1,True)\n",
        "        \n",
        "    \n",
        "\n",
        "      # update weights (again cross multiplication , added to all weights between the two matrix)\n",
        "        #syn0 += np.dot(l0.T,l1_delta)\n",
        "     # return the prediction:\n",
        "    return nonlin(total)\n",
        "      \n",
        "w,l1 = neuralNet(X,1000, False, testWeights = 2*np.random.random((3,1)) - 1)\n",
        "print(\"Output After Training:\")\n",
        "print(str(l1))\n",
        "print(\"Weights: \" + str(w))\n",
        "#user test:\n",
        "userInput = input(\"Enter int array: \")\n",
        "userArray = list(map(int, userInput.split()))\n",
        "print(userArray)\n",
        "print(str(neuralNet(X,1,True, userTest=userArray, testWeights=w)))\n",
        "#Note that adding a 0 0 0 learning set and a 0 1 0 doesn't work yet, we need to introduce two hyperparameters, layers, numbers of neurons, and an alpha or (learning rate)\n",
        "#Try 1 1 0 which is not in the data set, but if assume the rule it's trying to infer is perfect correspondence to the first column should return 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error before slope of sigmoid of values in l1\n",
            "[[-0.2689864 ]\n",
            " [-0.36375058]\n",
            " [ 0.76237183]\n",
            " [ 0.6737243 ]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.2689864 ]\n",
            " [0.36375058]\n",
            " [0.23762817]\n",
            " [0.3262757 ]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.05289153]\n",
            " [-0.08418501]\n",
            " [ 0.13811206]\n",
            " [ 0.14809799]]\n",
            "error before slope of sigmoid of values in l1\n",
            "[[-0.03178421]\n",
            " [-0.02576499]\n",
            " [ 0.02093318]\n",
            " [ 0.02585355]]\n",
            "l1 itself (dot product of l0 and weight (weighted sum))\n",
            "[[0.03178421]\n",
            " [0.02576499]\n",
            " [0.97906682]\n",
            " [0.97414645]]\n",
            "error after slope of sigmoid of values in l1 is multiplied times l1 error\n",
            "[[-0.00097813]\n",
            " [-0.00064673]\n",
            " [ 0.00042903]\n",
            " [ 0.00065113]]\n",
            "Output After Training:\n",
            "[[0.03178421]\n",
            " [0.02576499]\n",
            " [0.97906682]\n",
            " [0.97414645]]\n",
            "Weights: [[ 7.26283009]\n",
            " [-0.21614618]\n",
            " [-3.41703015]]\n",
            "Enter int array: 0 1 1\n",
            "[0, 1, 1]\n",
            "[0.02575143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO--FYBqxxwQ",
        "colab_type": "code",
        "outputId": "c9805ed2-c602-4de7-efad-adcdde307c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def nonlin(x,deriv=False):\n",
        "\tif(deriv==True):\n",
        "\t    return x*(1-x)\n",
        "\n",
        "\treturn 1/(1+np.exp(-x))\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "# randomly initialize our weights with mean 0\n",
        "syn0 = 2*np.random.random((3,4)) - 1\n",
        "syn1 = 2*np.random.random((4,1)) - 1\n",
        "\n",
        "for j in range(60000):\n",
        "\n",
        "\t# Feed forward through layers 0, 1, and 2\n",
        "    l0 = X\n",
        "  # It's really just 2 of the previous implementation stacked on top of each other. The output of the first layer (l1) is the input to the second layer.\n",
        "    l1 = nonlin(np.dot(l0,syn0))\n",
        "    l2 = nonlin(np.dot(l1,syn1))\n",
        "\n",
        "    # how much did we miss the target value?\n",
        "    l2_error = y - l2\n",
        "    \n",
        "    if (j% 10000) == 0:\n",
        "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
        "        \n",
        "    # in what direction is the target value?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
        "\n",
        "    # how much did each l1 value contribute to the l2 error (according to the weights)? This is a critical step!\n",
        "    # use the \"confidence weighted error\" from l2 to establish an error for l1. To do this, send the error across the weights from l2 to l1. \n",
        "    # This gives what you could call a \"contribution weighted error\" because we learn how much each node value in l1 \"contributed\" to the error in l2. This step is called \"backpropagating\" and is the namesake of the algorithm. We then update syn0 using the same steps we did in the 2 layer implementation.\n",
        "    l1_error = l2_delta.dot(syn1.T)\n",
        "    \n",
        "    # in what direction is the target l1?\n",
        "    # were we really sure? if so, don't change too much.\n",
        "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
        "\n",
        "    syn1 += l1.T.dot(l2_delta)\n",
        "    syn0 += l0.T.dot(l1_delta)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:0.4964100319027255\n",
            "Error:0.008584525653247157\n",
            "Error:0.0057894598625078085\n",
            "Error:0.004629176776769985\n",
            "Error:0.0039587652802736475\n",
            "Error:0.003510122567861678\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9Y_QGaC6rSA",
        "colab_type": "code",
        "outputId": "b7bbed06-a0d0-4fee-ddfa-2776886592ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "# input dataset\n",
        "X = np.array([  [0,1],\n",
        "                [0,1],\n",
        "                [1,0],\n",
        "                [1,0] ])\n",
        "    \n",
        "# output dataset            \n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "synapse_0 = 2*np.random.random((2,1)) - 1\n",
        "\n",
        "for iter in range(10000):\n",
        "\n",
        "    # forward propagation\n",
        "    layer_0 = X\n",
        "    layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "\n",
        "    # how much did we miss?\n",
        "    layer_1_error = layer_1 - y\n",
        "\n",
        "    # multiply how much we missed by the \n",
        "    # slope of the sigmoid at the values in l1\n",
        "    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "    synapse_0_derivative = np.dot(layer_0.T,layer_1_delta)\n",
        "\n",
        "    # update weights\n",
        "    synapse_0 -= synapse_0_derivative\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "print(layer_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output After Training:\n",
            "[[0.00505119]\n",
            " [0.00505119]\n",
            " [0.99494905]\n",
            " [0.99494905]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9tWC0byeBEY",
        "colab_type": "code",
        "outputId": "4fd9ac91-469a-48a9-83ef-1db1dc46aef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "#Challenge: find the best hyperparameters from alpha and hidden layer sizes\n",
        "#Bonus extend this to an artibray user enter number of hidden layers with user entered sizes\n",
        "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
        "hiddenSizes = [4,8,16,32]\n",
        "\n",
        "# compute sigmoid nonlinearity\n",
        "def sigmoid(x):\n",
        "    output = 1/(1+np.exp(-x))\n",
        "    return output\n",
        "\n",
        "# convert output of sigmoid function to its derivative\n",
        "def sigmoid_output_to_derivative(output):\n",
        "    return output*(1-output)\n",
        "    \n",
        "X = np.array([[0,0,1],\n",
        "            [0,1,1],\n",
        "            [1,0,1],\n",
        "            [1,1,1]])\n",
        "                \n",
        "y = np.array([[0],\n",
        "\t\t\t[1],\n",
        "\t\t\t[1],\n",
        "\t\t\t[0]])\n",
        "\n",
        "for alpha in alphas:\n",
        "  for hiddenSize in hiddenSizes:\n",
        "    print(\"\\nTraining With Alpha:\" + str(alpha) + \" and hidden layer size: \" + str(hiddenSize))\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # randomly initialize our weights with mean 0\n",
        "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
        "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
        "\n",
        "    for j in range(60000):\n",
        "\n",
        "        # Feed forward through layers 0, 1, and 2\n",
        "        layer_0 = X\n",
        "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
        "\n",
        "        # how much did we miss the target value?\n",
        "        layer_2_error = layer_2 - y\n",
        "\n",
        "        if (j % 20000) == 0:\n",
        "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
        "\n",
        "        # in what direction is the target value?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
        "\n",
        "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
        "\n",
        "        # in what direction is the target l1?\n",
        "        # were we really sure? if so, don't change too much.\n",
        "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
        "\n",
        "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
        "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.4935960431880486\n",
            "Error after 40000 iterations:0.48910016654420474\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.4960339829421634\n",
            "Error after 40000 iterations:0.49230609024302613\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.4915701638369947\n",
            "Error after 40000 iterations:0.4835511920051555\n",
            "\n",
            "Training With Alpha:0.001 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.48497630702745953\n",
            "Error after 40000 iterations:0.46903846539028254\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.359097202563399\n",
            "Error after 40000 iterations:0.14307065901337032\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.33871559263379203\n",
            "Error after 40000 iterations:0.11085529399713788\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.2378006592135859\n",
            "Error after 40000 iterations:0.090147768216335\n",
            "\n",
            "Training With Alpha:0.01 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.14693984546475994\n",
            "Error after 40000 iterations:0.06514781927504919\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.024098994228521613\n",
            "Error after 40000 iterations:0.014987616272210912\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.024038515823849808\n",
            "Error after 40000 iterations:0.015353573121544378\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.02386891164981858\n",
            "Error after 40000 iterations:0.015406466690561896\n",
            "\n",
            "Training With Alpha:0.1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.019063872533418433\n",
            "Error after 40000 iterations:0.012389242990471293\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0057894598625078085\n",
            "Error after 40000 iterations:0.0039587652802736475\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.006064058766005624\n",
            "Error after 40000 iterations:0.0041540985823973085\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.006081688500988915\n",
            "Error after 40000 iterations:0.004147665245792007\n",
            "\n",
            "Training With Alpha:1 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.004972517050388162\n",
            "Error after 40000 iterations:0.0033864102198316558\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.0021445955798521767\n",
            "Error after 40000 iterations:0.001478214512290799\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.0015995429469240542\n",
            "Error after 40000 iterations:0.0011113112681715655\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.0016467187353623783\n",
            "Error after 40000 iterations:0.001143653434163994\n",
            "\n",
            "Training With Alpha:10 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.001532635713923703\n",
            "Error after 40000 iterations:0.001054907732624855\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.12533033352910083\n",
            "Error after 40000 iterations:0.12523107366284103\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5000008759677884\n",
            "Error after 40000 iterations:0.5000009297529785\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:100 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 4\n",
            "Error after 0 iterations:0.4964100319027255\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 8\n",
            "Error after 0 iterations:0.49885891282661\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 16\n",
            "Error after 0 iterations:0.4968197940374576\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n",
            "\n",
            "Training With Alpha:1000 and hidden layer size: 32\n",
            "Error after 0 iterations:0.49643992250078794\n",
            "Error after 20000 iterations:0.5\n",
            "Error after 40000 iterations:0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbOt2kw1ygnB"
      },
      "source": [
        "**Classifying Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL3OqFKZ9dFg",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dzLKpmZICaWN",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7MqDQO0KCaWS",
        "colab": {}
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjnLH5S2CaWx",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zW5k_xz1CaWX",
        "colab": {}
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRFYHB2mCaWb",
        "colab": {}
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKnCTHz4CaWg",
        "colab": {}
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KFnYlcwCaWl",
        "colab": {}
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJmPr5-ACaWn",
        "colab": {}
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4VEw8Ud9Quh",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bW5WzIPlCaWv",
        "colab": {}
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oZTImqg_CaW1",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ODch-OFCaW4",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lhan11blCaW7",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvwvpA64CaW_",
        "colab": {}
      },
      "source": [
        "model.fit(train_images, train_labels, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VflXLEeECaXC",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gl91RPhdCaXI",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3DmJEUinCaXK",
        "colab": {}
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qsqenuPnCaXO",
        "colab": {}
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sd7Pgsu6CaXP",
        "colab": {}
      },
      "source": [
        "test_labels[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DvYmmrpIy6Y1",
        "colab": {}
      },
      "source": [
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HV5jw-5HwSmO",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ko-uzOufSCSe",
        "colab": {}
      },
      "source": [
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hQlnbqaw2Qu_",
        "colab": {}
      },
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRJ7JU7JCaXT",
        "colab": {}
      },
      "source": [
        "# Grab an image from the test dataset.\n",
        "img = test_images[1]\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lDFh5yF_CaXW",
        "colab": {}
      },
      "source": [
        "# Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_rzNSdrCaXY",
        "colab": {}
      },
      "source": [
        "predictions_single = model.predict(img)\n",
        "\n",
        "print(predictions_single)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Ai-cpLjO-3A",
        "colab": {}
      },
      "source": [
        "plot_value_array(1, predictions_single[0], test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tRmdq_8CaXb",
        "colab": {}
      },
      "source": [
        "np.argmax(predictions_single[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PomxRyWRzR4E",
        "colab_type": "text"
      },
      "source": [
        "**Classifying Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nh0KjNGMWNlL",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8RZOuS9LWQvv",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wbIQ2wSeXSme",
        "colab": {}
      },
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    # Use the version pre-encoded with an ~8k vocabulary.\n",
        "    'imdb_reviews/subwords8k', \n",
        "    # Return the train/test datasets as a tuple.\n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "    as_supervised=True,\n",
        "    # Also return the `info` structure. \n",
        "    with_info=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EplYp5pNnW1S",
        "colab": {}
      },
      "source": [
        "encoder = info.features['text'].encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7ACuHM5hFp3",
        "colab": {}
      },
      "source": [
        "print ('Vocabulary size: {}'.format(encoder.vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq6xDmf2SAs-",
        "colab": {}
      },
      "source": [
        "sample_string = 'Hello TensorFlow.'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print ('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print ('The original string: \"{}\"'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GUIRWSO8yxT5",
        "colab": {}
      },
      "source": [
        "for ts in encoded_string:\n",
        "  print ('{} ----> {}'.format(ts, encoder.decode([ts])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxnWQJijdGA1",
        "colab": {}
      },
      "source": [
        "for train_example, train_label in train_data.take(1):\n",
        "  print('Encoded text:', train_example[:10].numpy())\n",
        "  print('Label:', train_label.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34VUXtgxsVpf",
        "colab": {}
      },
      "source": [
        "encoder.decode(train_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SDRI_s_tX1Hk",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = (\n",
        "    train_data\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .padded_batch(32, train_data.output_shapes))\n",
        "\n",
        "test_batches = (\n",
        "    test_data\n",
        "    .padded_batch(32, train_data.output_shapes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXXne4DreQfv",
        "colab": {}
      },
      "source": [
        "for example_batch, label_batch in train_batches.take(2):\n",
        "  print(\"Batch shape:\", example_batch.shape)\n",
        "  print(\"label shape:\", label_batch.shape)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpKOoWgu-llD",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(encoder.vocab_size, 16),\n",
        "  keras.layers.GlobalAveragePooling1D(),\n",
        "  keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mr0GP-cQ-llN",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXSGrjWZ-llW",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_batches,\n",
        "                    epochs=10,\n",
        "                    validation_data=test_batches,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOMKywn4zReN",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate(test_batches)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VcvSXvhp-llb",
        "colab": {}
      },
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nGoYf2Js-lle",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6hXx-xOv-llh",
        "colab": {}
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}